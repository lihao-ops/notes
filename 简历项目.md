







## 项目

### 量化选股项目



#### **量化选股系统面试讲法（示例稿）**

##### 1️⃣ 项目概述（30-40秒）

> “我参与的项目是一个量化选股与策略回测系统，基于 Spring Cloud 微服务架构开发。
>  系统主要模块包括：数据采集与存储、因子计算、选股策略、回测引擎、公共 Base 模块和风控模块。
>  数据采集模块使用 MySQL 存储历史行情和因子数据，Redis 做缓存和布隆过滤器防穿透，Kafka 负责行情推送、任务分发和日志分流。
>  系统支持高并发请求，采用 Sentinel 做熔断与限流，微服务间通信通过 OpenFeign + Nacos 做服务注册与配置管理。”

------

##### 2️⃣ 我的职责（20秒）

> “我主要负责 **数据采集模块和回测模块** 的实现，包括：
>
> - MySQL 表结构设计
> - Redis 缓存及布隆过滤器实现
> - Kafka 消息处理
> - 回测引擎性能优化
>    我同时负责系统整体吞吐量和响应性能的提升。”

------

##### 3️⃣ 技术亮点（重点，面试常问，1-2分钟）

### (1) Redis 缓存 + 布隆过滤器 + 分布式锁

- **缓存防穿透**：布隆过滤器判断数据是否存在，避免频繁查询数据库
- **任务幂等性**：用户重复请求（例如查询策略回测结果）时，通过分布式锁（key=userId+接口mapping）控制只有一个实例执行请求，其他请求直接返回“正在处理中”
- **实现方式**：

```text
用户第一次请求 -> 尝试加锁(setnx) -> 执行任务 -> 布隆过滤器记录 -> 返回结果 -> 释放锁
用户第二次请求 -> 查询布隆过滤器存在 -> 直接返回“请稍等”
```

### (2) Kafka 消息队列

- 任务分发、行情推送和日志分流
- 多 partition + leader/replica 保证吞吐量与高可用
- 异步解耦，支撑日任务峰值流量 1.5 倍

### (3) MyBatis 慢 SQL 监控

- 使用 `Interceptor` 拦截 SQL，打印慢 SQL 并告警
- 提高数据库性能可观测性

### (4) JVM 异常监控

- 定时检测内存和线程指标
- 预警 OOM / StackOverflow，避免服务崩掉

------

##### 4️⃣ 性能成果（用数据说话）

- 系统吞吐量提升 **1.8 倍**
- 单次响应延迟降低 **40%**
- 支持万级并发，峰值流量下系统仍可承载 **原有吞吐量的 1.5 倍**
- 成本保持稳定，整体性能和稳定性显著提升

------

##### 5️⃣ 面试追问应答（示例）

| 面试官可能问                 | 你可以回答                                                   |
| ---------------------------- | ------------------------------------------------------------ |
| 为什么用布隆过滤器？         | 可以快速判断元素是否存在，防止缓存穿透，减少数据库压力       |
| 分布式锁如何保证幂等？       | lock key=userId+接口，第一次加锁执行任务，其他重复请求直接返回“处理中”，完成后释放锁 |
| Kafka 为什么用多 partition？ | 提升并发消费能力，每个 partition 可以独立处理消息，结合副本保证高可用 |
| MyBatis 慢 SQL 怎么监控？    | 使用 `Interceptor` 拦截 SQL 执行，记录耗时，超过阈值告警     |
| OOM/栈异常如何预警？         | 使用 `MemoryMXBean` 和 `ThreadMXBean` 定时检测，发现异常发送日志/告警，避免服务崩掉 |

------

✅ **讲法技巧**

1. 避免“从头到尾技术堆砌”，用**问题-方案-结果**讲法
2. 核心技术点突出：Redis 布隆过滤器 + 分布式锁 + Kafka + 慢 SQL + JVM 监控
3. 用数据支持成果，面试官最喜欢
4. 面试中遇到追问，不慌，围绕你实现过的功能和原理展开即可





#### 布隆过滤器



##### 什么是布隆过滤器

布隆过滤器是一种**空间和时间效率极高的概率型数据结构**，用于判断一个元素是否在一个集合中。

| 特性             | 描述                                                         |
| ---------------- | ------------------------------------------------------------ |
| **查询效率高**   | O(k) 时间复杂度（k 是哈希函数数量）                          |
| **空间效率高**   | 使用位数组，不存储实际元素                                   |
| **支持批量检测** | 可以快速判断大量元素是否存在                                 |
| **可能存在误判** | `mightContain` 有一定概率返回 `true` 但元素实际上不存在（称为假阳性） |
| **绝不会漏判**   | 如果返回 `false`，元素肯定不存在                             |



##### 应用场景

- 缓存穿透防护：数据库中不存在的数据，不重复查询数据库
- 去重：如爬虫 URL 去重
- 邮件黑名单、分布式系统的快速存在性判断



##### 实现过程

布隆过滤器本质上是`位数组` + `多个哈希函数`：

1. 初始化

   1. 分配一块固定长度的位数组（bitmap）
   2. 选择k个哈希函数(hash1,hash2...hashk)

2. 添加元素(add)

   1. 对元素计算k个哈希值

   2. 对应位数组位置是1

   3. ```test
      hash1(element) -> index1 -> bitmap[index1] = 1
      hash2(element) -> index2 -> bitmap[index2] = 1
      ...
      hashk(element) -> indexk -> bitmap[indexk] = 1
      ```

3. 查询元素(`mightContain`)

   1. 对元素计算同样的k个哈希值
   2. 检查位数组对应位置是否全为1
      1. 全为1 ——> 可能存在(存在假阳性)
      2. 有0 ——> 一定不存在

4. 优点与限制

   1. 优点：极少内存即可处理大量元素，高并发查询快
   2. 限制：不能删除单个元素(标准布隆过滤器)，存在误判率。



##### 在 Redis 上的实现

Redis 的布隆过滤器一般用 **位图（BITMAP） + Lua脚本或 RedisBloom 模块**：

- **位图存储**：每个元素用多个哈希映射到 bitmap 的不同 bit
- **增加元素**：`SETBIT key offset 1`
- **查询元素**：`GETBIT key offset`
- **删除元素**：标准布隆不支持，使用 **计数型布隆（Counting Bloom）** 可以支持

Java 实现流程：

1. `add(key, value)` → 根据 k 个哈希函数计算偏移量 → Redis `SETBIT`
2. `mightContain(key, value)` → 计算偏移量 → Redis `GETBIT`
3. 利用 Redis 高效的 bitmap 数据结构，支持大规模分布式存储



##### 实现代码

```java
   // 判断 key 是否可能存在
   if (redisBloomFilter.mightContain("bloom:data", key)) {
       // 可能存在 → 访问缓存或数据库
   } else {
       // 一定不存在 → 拦截请求
   }

   // 添加新的 key 到布隆过滤器
   redisBloomFilter.add("bloom:data", key);
```





#### Redis分布式锁实现幂等

```mermaid
flowchart TD
    A[用户请求大模型接口] --> B{布隆过滤器是否存在 key?}
    B -- 是 --> C[返回 请求正在处理中,请稍等]
    B -- 否 --> D[尝试获取分布式锁 key = 用户ID + 接口映射]
    D -- 成功 --> E[开始执行请求任务]
    E --> F[任务执行完成,返回响应结果]
    F --> G[释放分布式锁并清理布隆过滤器标记]
    D -- 失败 --> C
```



1. 用户请求大模型返回非流式结果时，某些数据需要大量计算，
2. 如果用户重复点击同一接口导致多个实例重复处理同一请求，
3. 可用分布式锁（key = 用户ID + 接口映射）保证首次请求执行，设置合理过期时间并标记到布隆过滤器，
4. 重复点击时通过布隆过滤器判断任务正在执行直接返回“正在处理”，
5. 任务完成后返回结果并清理标记，从而保证任务幂等性和资源安全。







##### **场景描述**

假设你在做一个量化交易系统或者股票策略回测系统：

- 用户发起**股票数据查询任务**
- 任务比较重（涉及调用第三方接口或计算指标）
- 系统部署在多节点微服务环境（Spring Boot + Redis 集群）

问题：

- 同一用户可能连续点击“查询”按钮
- 微服务多实例部署 → 可能同时触发多次同样的查询任务
- 造成 **重复计算、重复调用第三方接口、浪费资源**

解决方法：**分布式锁 + 幂等设计**

------

##### **实现流程**

1. **定义唯一任务标识**

   - 比如 `userId:stockCode:date`
   - 例子：`1001:AAPL:2025-09-20`

2. **尝试获取分布式锁**（基于 Redis）

   ```java
   String lockKey = "stock:query:lock:" + userId + ":" + stockCode + ":" + date;
   String lockValue = UUID.randomUUID().toString(); // 锁的唯一标识
   boolean acquired = redisConfig.tryLock(lockKey, lockValue, 10); // 10秒过期
   if (!acquired) {
       return "任务正在执行，请稍后再试";
   }
   ```

3. **检查任务是否已经执行过**（幂等）

   - 可以在 Redis 布隆过滤器或普通 key 保存执行标记

   ```java
   String bloomKey = "stock:query:executed";
   if (redisBloomFilter.mightContain(bloomKey, lockKey)) {
       return "任务已执行，无需重复查询";
   }
   ```

4. **执行任务**

   - 查询数据库或调用第三方接口
   - 将结果缓存

5. **标记任务已执行**

   ```java
   redisBloomFilter.add(bloomKey, lockKey);
   ```

6. **释放分布式锁**

   ```java
   redisConfig.releaseLock(lockKey, lockValue);
   ```

------

##### **效果**

| 功能           | 说明                                       |
| -------------- | ------------------------------------------ |
| 分布式锁       | 多实例下同一任务同时执行只允许一个         |
| 幂等性         | 即使用户重复请求，也只计算一次             |
| 结合布隆过滤器 | 防止缓存穿透，同时快速判断任务是否已经完成 |

------

###### **简化 mermaid 流程图（股票查询幂等 + 分布式锁）**

```mermaid
flowchart TD
    A[用户点击查询按钮] --> B{检查任务是否在执行?}
    B -- 是 --> C[返回 任务正在执行]
    B -- 否 --> D[尝试获取Redis分布式锁]
    D -- 成功 --> E[检查布隆过滤器是否已执行]
    E -- 已执行 --> F[返回 任务已执行 ]
    E -- 未执行 --> G[执行查询任务]
    G --> H[缓存结果]
    H --> I[标记布隆过滤器已执行]
    I --> J[释放分布式锁]
    J --> K[返回查询结果]
    D -- 失败 --> C
```

------

##### ✅ **总结**

- 分布式锁：保证同一任务在多实例环境下不会被重复执行
- 幂等设计：保证重复请求不会重复计算
- 布隆过滤器/Redis：高效记录已执行任务，防止缓存穿透

------













#### 任务分发 + 日志分流 + 慢 SQL 监控 + JVM 监控

```mermaid
flowchart TD
    %% 应用层
    A[应用服务 Spring Boot] -->|执行 SQL| B[MyBatis/P6Spy 拦截器]
    A -->|业务任务| F[任务分发 Kafka/RabbitMQ]

    %% SQL 监控
    B -->|记录 SQL 执行时间| C[慢 SQL 判断 >3s]
    C -->|慢 SQL| D[日志系统 / 告警系统 Kafka/ELK]
    C -->|正常 SQL| E[日志系统 / 分析平台]

    %% JVM 监控
    G[JVM 监控定时任务] --> H{检查堆/非堆内存 & 线程栈}
    H -->|超过阈值| D
    H -->|正常| I[正常运行日志]

    %% 任务分发
    F --> J[各服务/线程消费处理]

    %% 日志分流
    E --> K[分析/存储/指标系统]
    D --> K
    I --> K

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:1px
    style C fill:#ffb,stroke:#333,stroke-width:1px
    style D fill:#f99,stroke:#333,stroke-width:2px
    style E fill:#9f9,stroke:#333,stroke-width:1px
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#fcf,stroke:#333,stroke-width:2px
    style H fill:#ffc,stroke:#333,stroke-width:1px
    style I fill:#cff,stroke:#333,stroke-width:1px
    style J fill:#cff,stroke:#333,stroke-width:1px
    style K fill:#eee,stroke:#333,stroke-width:1px

```



✅ **说明**：

1. **应用服务**：业务逻辑执行 SQL 或发送任务
2. **SQL 拦截器**：监控慢 SQL（>3s），分流日志
3. **JVM 监控**：定时任务检查堆/非堆内存和线程栈，异常触发告警
4. **日志系统**：慢 SQL、异常日志告警或存储分析
5. **任务分发**：Kafka/RabbitMQ 将业务任务分发到独立服务或线程
6. **分析/存储/指标系统**：集中存储 SQL、日志、指标，可用于报表或告警





##### 系统可观测性与稳定性提升方案概述

###### 1️⃣ 论述

在高并发、微服务场景下，系统的**可观测性**和**稳定性**直接影响业务可靠性。
 主要关注点包括：

1. **任务分发**：合理分配系统任务或消息，确保各服务/线程/节点负载均衡，提高响应效率并避免单点压力过大。
2. **日志分流**：对不同类型日志（ERROR、INFO、指标、慢 SQL 等）进行分类路由，分别送往告警系统、分析平台或归档存储。
3. **慢 SQL 监控与日志采集**：通过拦截器（MyBatis Interceptor 或 P6Spy）实时记录 SQL 执行时间和参数，对超过阈值的慢 SQL 进行告警或日志收集，为性能优化提供依据。
4. **JVM/系统监控**：实时监控堆内存、非堆内存、线程数和栈使用，预警潜在 OOM 或栈溢出风险，保障系统稳定性。

------

###### 2️⃣ 实现方式（Spring Boot 为例）

| 功能        | 实现方式                                  | 目的                                                |
| ----------- | ----------------------------------------- | --------------------------------------------------- |
| 任务分发    | Kafka / RabbitMQ 等消息队列               | 解耦服务，提高并发处理能力                          |
| 日志分流    | Logback + AsyncAppender 或 Kafka/Logstash | 将日志按类型分发到不同存储/处理端                   |
| 慢 SQL 监控 | MyBatis 拦截器（Interceptor）或 P6Spy     | 记录 SQL 执行时间、参数，超过阈值触发告警           |
| JVM 监控    | 定时任务 + MemoryMXBean/ThreadMXBean      | 预警堆/非堆内存、线程栈异常，防止 OOM/StackOverflow |

------

###### 3️⃣ 整体流程示意（文字版）

```
应用服务：
    ↓ (SQL 执行)
MyBatis 拦截器 / P6Spy
    ↓ (慢 SQL)
日志系统 → 告警 / 分析平台 (Kafka/ELK)
    ↓
定时 JVM 监控 → WARN 日志 / 告警
    ↓
任务分发 → 各服务/线程消费处理
```

------

###### 4️⃣ 总结

通过**任务分发 + 日志分流 + 慢 SQL 监控 + JVM 监控**：

1. **解耦**：服务间压力独立，任务和日志分流处理效率更高
2. **可观测性**：SQL 执行、系统资源、线程栈等关键指标可实时监控
3. **稳定性**：提前预警慢 SQL、OOM 或线程异常，降低系统故障风险

整体效果：系统运行透明、可分析、可扩展，既保障业务连续性，又为优化提供数据支撑。

------





```yaml
# Kafka 配置（生产者侧）
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      # 序列化方式：key/value 都用字符串
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer

      # 消息可靠性相关
      acks: all                          # 等待所有 ISR 副本确认（最安全，消息不丢）
      retries: 2147483647                # 无限重试（保证 leader 临时挂掉、网络抖动时也不会丢消息）
      enable-idempotence: true           # 开启幂等性，避免重试时产生重复消息
      max-in-flight-requests-per-connection: 5  # 保证幂等性的同时允许并发（推荐 5）

      # 批量与延迟相关
      linger-ms: 5                       # 等待 5ms 批量发送（延迟可控，吞吐更好；若对实时性要求极高可改回 0）
      batch-size: 32768                  # 每个分区的批量大小（32KB），提高网络传输效率
      buffer-memory: 67108864            # 64MB 全局缓冲区，容纳未发送消息

      # 压缩配置
      compression-type: snappy           # 压缩算法（snappy 在 CPU 开销和压缩率之间平衡，适合行情数据）

```



#### 幂等性



##### 什么是幂等性？

>幂等性(ldempotence)是分布式系统和消息系统里的一个核心概念

一次或多次执行同样的操作，产生的结果是一样的，不会因为重复执行而引起副作用。

比如：

- `推送贵州茅台的行情数据`

- `PUT /user/123 {"name":"小李"}` → 不管你调用一次还是调用 100 次，用户的 name 最终都是「小李」。

  转账操作如果没有幂等控制，你可能因为网络重试，用户的钱被扣了两次。



##### 在kafka里的幂等性

> kafka Producer默认情况：

- 网络抖动、ACK超时 ——> Producer可能会**重试**。
- 如果Broker已经写成功，但是Producer没收到ACK，它会再发一次，结果就是**重复消息**。



##### 幂等生产者(ldempotent Producer)

>为了解决这个问题，kafka在0.11版本引入了**幂等生产者（Idempotent Producer）**：

- 打开参数：`enable.idempotence=true`(从kafka3.X开始默认就是`true`)
- `kafka`给每个Producer分配一个`Producer ID`，并给每条消息加上一个`序列号(Sequence Namber)`
- `Broker`会检测`(PID, Partition, SequenceNumber)` 是否重复，重复的消息就会被丢弃，保证"至少一次"——>"恰好一次"(在单个分区内)。



##### kafka幂等时序图

```mermaid
sequenceDiagram
    participant Producer
    participant Broker(Leader Partition)
    participant Follower1
    participant Follower2

    Note over Producer: Producer 启动，申请 PID
    Producer->>Broker(Leader Partition): InitPID 请求
    Broker(Leader Partition)-->>Producer: 分配 ProducerID

    Note over Producer: 发送消息时带上 (PID,Partition,SequenceNumber)
    Producer->>Broker(Leader Partition): Send[PID=101,Seq=1,Msg=A]
    Broker(Leader Partition)->>Follower1: Replicate Msg=A
    Broker(Leader Partition)->>Follower2: Replicate Msg=A
    Follower1-->>Broker(Leader Partition): ACK
    Follower2-->>Broker(Leader Partition): ACK
    Broker(Leader Partition)-->>Producer: ACK 成功

    Note over Producer: 网络抖动，Producer 没收到 ACK，重发
    Producer->>Broker(Leader Partition): Resend[PID=101,Seq=1,Msg=A]
    Broker(Leader Partition)-->>Producer: 发现重复序列号，丢弃消息
    Broker(Leader Partition)-->>Producer: ACK 幂等确认

```











👌我帮你更新了笔记，把你最新的 **ACK=all 幂等配置后的测试结果**替换进去了：

------

#### kafka推送行情数据

##### 1️⃣ 单机测试数据回顾（开启 acks=all + 幂等性）

| 指标         | 单机测试值                        |
| ------------ | --------------------------------- |
| Broker 数量  | 1                                 |
| Topic        | 1                                 |
| Partition    | 1                                 |
| Batch Size   | 300                               |
| 目标 TPS     | 1800 条/秒                        |
| 总发送条数   | 324,000                           |
| 总耗时       | 181,953 ms ≈ 182 秒               |
| 实际平均 TPS | **1,780.68 ≈ 1800 条/秒**         |
| 峰值 TPS     | **45,000**（单 batch 短时间爆发） |

------

##### 2️⃣ 单机 TPS 曲线（Mermaid）

```mermaid
%% 单机 TPS 曲线（观测值）
%% 横轴：时间（秒），纵轴：TPS
%% 注：批量发送时短时间会有高峰，平均稳定在 1800 TPS
gantt
    title 单机 TPS 曲线（批量发送特征）
    dateFormat  SS
    axisFormat  %S
    section TPS
    TPS_Avg_1800 :active, 0, 182
    TPS_Peak_45000 :crit, 0, 1
```

------

##### 3️⃣ 生产环境推算（3 Broker 集群）

###### 3.1 Partition 叠加示意（Mermaid Flow）

```mermaid
flowchart LR
    Partition1[Partition 1<br/>TPS约等于1800] --> TotalTPS[总 TPS约等于5400]
    Partition2[Partition 2<br/>TPS约等于1800] --> TotalTPS
    Partition3[Partition 3<br/>TPS约等于1800] --> TotalTPS
```

###### 3.2 系统结构示意（Mermaid）

```mermaid
graph TD
    subgraph Kafka Cluster
        Broker1[Broker 1]
        Broker2[Broker 2]
        Broker3[Broker 3]
    end
    Producer[数据生产模块] --> Broker1
    Producer --> Broker2
    Producer --> Broker3
    Broker1 --> ConsumerGroup[消费组]
    Broker2 --> ConsumerGroup
    Broker3 --> ConsumerGroup
    Redis[Redis 集群+布隆过滤器] --> ConsumerGroup
    MySQL[MySQL] --> ConsumerGroup
```

------

##### 4️⃣ 生产环境数值表

| 指标             | 计算/假设                 | 结果                    |
| ---------------- | ------------------------- | ----------------------- |
| Broker 数量      | 3                         | 3                       |
| Topic            | 1                         | 1                       |
| Partition        | 每 broker 1 个            | 3                       |
| 单 partition TPS | 单机测试值                | ~1800                   |
| 总 TPS           | 3 × 1800                  | ~5400                   |
| 单秒网络传输量   | 5400 × 1 KB               | 5.4 MB/s                |
| 消息写入延迟     | Kafka batch + replication | 1~2 ms                  |
| 总体系统延迟     | 消息发送完成              | <5 ms                   |
| 批量发送         | batch.size = 300          | 提升吞吐，降低 CPU 开销 |
| 持续 3 分钟负载  | 5400 TPS × 180 秒         | 972,000 条              |

------

##### 5️⃣ 面试口述模板

> “我在单机实验中按每秒 1800 条发送历史行情数据，总耗时约 182 秒，平均 TPS ≈ 1800。
>  同时观测到由于 batch 提交，瞬时峰值可达到 4.5 万 TPS。
>  推算到线上 3 broker 集群、3 partition，总 TPS 约 5400 条/秒，网络延迟仍可控。
>  我使用了 `acks=all + 幂等性` 配置，确保消息不丢失不重复；批量发送进一步提升吞吐，系统稳定支撑日峰值流量。
>  在项目中，我结合 Spring Cloud + Nacos 做服务注册与配置管理，OpenFeign 实现微服务调用，Sentinel 做熔断与限流，Redis 集群 + 布隆过滤器解决缓存穿透，分布式锁保证任务幂等性。
>  Kafka 负责行情推送、任务分发与日志分流，配合慢 SQL 监控和日志采集，系统整体性能与稳定性显著提升。”

------

要不要我再给你补一张 **“ACK=1 vs ACK=all 性能对比图（Mermaid 柱状图）”**，这样面试时更有直观效果？











```mermaid
flowchart LR
    subgraph 单 partition
        P1[Partition 1<br/>TPS 约等于 1800]
    end

    subgraph 三 partition 生产环境
        P1_Prod[Partition 1<br/>TPS 约等于 1800]
        P2_Prod[Partition 2<br/>TPS 约等于 1800]
        P3_Prod[Partition 3<br/>TPS 约等于 1800]
    end

    P1 --> SingleTPS[总 TPS 约等于 1800<br/>单机实验]
    P1_Prod --> TotalTPS[总 TPS 约等于 5400<br/>叠加吞吐量]
    P2_Prod --> TotalTPS
    P3_Prod --> TotalTPS

```



------

#### Kafka 消息处理优化（生产环境版）

##### ✅ 技术概览

- Kafka 用作行情推送、任务分发和日志分流。
- 与 Spring Cloud + Nacos 配合，实现微服务注册与配置管理。
- 配合 OpenFeign 做服务调用，Sentinel 做熔断与限流。
- 目标：高可靠、高吞吐、低延迟、可观测性强。

------

##### ✅ 关键优化点

- **批量发送**：batch.size = 300 → 降低 CPU 开销，提升吞吐。
- **幂等性 + 高可靠**：
  - `acks=all` → 等待所有 ISR 副本确认，保证消息不丢失。
  - `enable-idempotence=true` → 幂等发送，避免重复消息。
  - `retries=Integer.MAX_VALUE` + `max.in.flight.requests.per.connection=5` → 无限重试同时保证顺序。
- **压缩和缓冲**：
  - `compression.type=snappy` → 减少网络带宽占用。
  - `buffer.memory=64MB` → 批量缓冲，提高吞吐。

------

##### ✅ 生产环境数值

| 指标              | 数值                   | 说明                     |
| ----------------- | ---------------------- | ------------------------ |
| Broker 数量       | 3                      | 生产环境                 |
| Topic / Partition | 1 / 3                  | 每 Broker 1 个 Partition |
| 单 Partition TPS  | 1800 条/秒             | 稳定发送速率             |
| 总 TPS            | 3 × 1800 = 5400 条/秒  | 集群总吞吐量             |
| 单秒网络传输量    | 5400 × 1 KB ≈ 5.4 MB/s | 消息平均大小假设 1 KB    |
| 消息写入延迟      | batch + replication    | 1~2 ms                   |
| 总体系统延迟      | 消息发送完成           | <5 ms                    |
| 批量发送效果      | batch.size = 300       | 提升吞吐，降低 CPU 开销  |

------

##### ✅ 简历论述模板

> “在生产环境中，我负责 Kafka 消息处理，支持行情推送、任务分发和日志分流。
>  系统采用批量发送（batch.size=300）+ 幂等性（enable-idempotence=true）+ 全副本确认（acks=all）+ 无限重试（retries=Integer.MAX_VALUE，max.in.flight.requests.per.connection=5），保证消息不丢失、不重复且顺序稳定。
>  集群配置为 3 个 Broker、3 个 Partition，每 Partition TPS ≈ 1800，总 TPS ≈ 5400 条/秒，单条消息延迟 <5ms，系统在峰值流量下仍能稳定支撑。
>  通过这些优化，系统整体吞吐量提升 1.5 倍，性能与可靠性显著提高。”

------







#### 问答

为什么要设置partition=3

1. **每个 Broker 一个 Partition**
   - Broker 只处理自己负责的 Partition 的 Leader，负载分散。
   - 对消息消费者来说，可以并行消费各个 Partition，提高并发处理能力。
2. **顺序保证**
   - Kafka 保证 Partition 内消息顺序，因此每个 Partition 不需要处理过多逻辑就能维持顺序。
   - 如果把所有消息都放在一个 Partition，顺序可以保证，但消费者只能单线程处理，吞吐受限。
3. **响应速度**
   - Partition 数量=Broker 数量时，Leader 分布均衡，Broker 并发处理压力小。
   - 这样即使消息量大，每个 Broker 的 IO、网络和 CPU 压力都相对低，响应快。
4. **可扩展性**
   - 以后增加 Broker 时，可以增加 Partition 数量，轻松提升总 TPS，而不影响 Partition 内顺序。

总结一句话：**Broker 数量=Partition 数量，既保证顺序，又分摊负载，提高响应速度和并发能力**。



> 1个集群3台实例，1个topic、3个partition、每个broker中，leader + 2个备份其它partition的副本。性价比很高。

1. **高可用**
   - 每个 Partition 都有 3 个副本（replication factor = 3），Leader + 2 个备份。
   - 即使其中 1 个 Broker 挂掉，剩余两个副本仍然可用，保证消息不丢失。
2. **并发吞吐**
   - Topic 有 3 个 Partition → 可以同时被 3 个消费者并行消费。
   - Broker 上的负载被均衡分配，每个 Broker 的 IO 压力合理。
3. **顺序保证**
   - 每个 Partition 内顺序保证，Partition 内的消费者处理顺序不乱。
4. **成本控制**
   - 3 个 Broker + 每个 Partition 3 个副本 → 资源消耗适中。
   - 副本数和 Partition 数不是越多越好，过多会增加 Broker 负载和网络开销。
5. **扩展性**
   - 后续想要更高吞吐 → 可以增加 Partition 数量或 Broker 数量。
   - 副本数保持 3，依然保证可用性和数据安全。

**总结**：

- 3 Broker、1 Topic、3 Partition、每 Partition 3 副本，是**低成本、高可用、可扩展、顺序保证的平衡配置**。
- 对于大部分行情推送场景，这个配置的性价比确实很高。

如果你愿意，我可以帮你画一张**Broker-Partition-Replica 的示意图**，直观展示 Leader 和副本分布，面试展示会更有说服力。你想要吗？





>日志分流

**使用消息队列分流**

如果你系统本身有 Kafka/RabbitMQ，可以把日志当作事件消息：

1. 应用写日志 → 发送到 Kafka Topic
2. 消费端按不同规则订阅不同 Topic

示意：

```
App → Kafka (log_topic)
Consumer1: 订阅 ERROR_TOPIC → 告警
Consumer2: 订阅 INFO_TOPIC → 分析
Consumer3: 订阅 METRICS_TOPIC → 指标存储
```

✅ 优点：

- 解耦明显
- 支持大流量
- 消费端可独立扩展

❌ 缺点：

- 增加开发复杂度
- 需要保证消息可靠性



| 维度         | Logback 写文件                 | Kafka/RabbitMQ 消息分流                 |
| ------------ | ------------------------------ | --------------------------------------- |
| **延迟**     | 很低（直接写文件）             | 稍高（网络 + Broker + 序列化）          |
| **吞吐量**   | 单机文件写入受磁盘 IO 限制     | 高吞吐，Kafka 单 broker 可轻松百万条/秒 |
| **解耦性**   | 应用耦合了存储方式             | 消费端独立，可横向扩展                  |
| **可扩展性** | 文件分片、切割有限             | Topic + Partition 设计可水平扩展        |
| **可靠性**   | 写文件容易丢失（崩溃或磁盘满） | Kafka/RabbitMQ 支持 ack、重试、持久化   |
| **复杂度**   | 低                             | 高，需要序列化、网络、消费端管理        |







### MySQL

#### MySQL数据表

##### 分时行情表主键 & 分表设计复盘

###### 背景
- 分时行情数据量大（约 5000 支股票 × 每秒一条 × 一天 ≈ 7000 万条）。
- 常见查询模式：**某只股票 + 时间范围**。
- 当前方案：
  - 主键：`(stock_code, timestamp)` 联合主键
  - 分表：按月分表（每月约 3000 万条）

---

###### 方案分析

✅ 优点

1. **查询模式匹配**  
   - `(stock_code, timestamp)` 作为联合主键，支持股票 + 时间范围查询，直接走聚簇索引，性能较好。
2. **单表可控**  
   - 按月分表，每表约 3000 万行，InnoDB 可接受，维护压力不大。
3. **天然唯一性**  
   - 主键同时保证数据唯一性，避免重复写入。

⚠️ 潜在问题

1. **索引冗余**  
   - InnoDB 聚簇索引是 `(code, timestamp)`，所有二级索引都会冗余这两个字段，导致存储膨胀。
2. **写入性能风险**  
   - 高并发写入时，联合主键并非严格单调递增，可能造成 **页分裂**，引发写入抖动。
3. **跨月查询复杂**  
   - 分表后，如果要查三个月的数据，需要查三张表再合并，业务层需要做路由和聚合。
4. **全市场横向查询不友好**  
   - 如果查询场景是「某一分钟内全市场所有股票快照」，会导致全表扫描，性能较差。

---

###### 改进思路

1. **主键优化**
   - 使用 **自增 ID / 雪花 ID** 作为主键，减小聚簇索引体积。
   - `(stock_code, timestamp)` 建立 **唯一索引**，保证业务唯一性。

2. **分库分表/分区**
   - 按 **交易日/交易月** 分表，保证单表数据量可控。
   - 或者按 **股票代码哈希** 分库，提升水平扩展能力。

3. **冷热数据分层**
   - 交易日内数据：留在 MySQL，支撑高频查询。
   - 历史数据：迁移至 **ClickHouse/HBase/Doris**，用于批量回测和分析。

4. **查询优化**
   - 纵向（单股票时间序列）：`INDEX(stock_code, trade_date, trade_time)`  
   - 横向（全市场快照）：可以引入 **列式存储**（ClickHouse/Doris），提升大规模聚合查询性能。

---

###### 面试回答模板

> 我们的典型查询场景是「某只股票 + 一段时间」，因此选择 `(stock_code, timestamp)` 作为联合主键，可以直接利用聚簇索引，查询性能较好。同时为了控制单表大小，我按月分表，每表约 3000 万行，保证了性能和运维的平衡。  
>
> 不过我也考虑过扩展性问题：  
> - 当数据量进一步增长时，我会把主键换成自增 ID，`(stock_code, timestamp)` 用唯一索引保证唯一性；  
> - 历史数据会做冷热分层，T+1 内留在 MySQL，历史迁移到 ClickHouse；  
> - 对跨月查询，会通过数据网关自动路由多表；  
> - 对全市场横向查询，可以利用列式存储做加速。  
>
> 这样既兼顾了 **当前查询效率**，又预留了 **未来扩展性**。

---

###### 面试官可能追问
1. 如果出现「每秒多笔行情」，如何避免 `timestamp` 冲突？  
2. 如果写入速率过高，MySQL 无法承受，如何优化写入架构？  
3. 如何支持「全市场一分钟快照」这类横向查询？  
4. 历史归档如何做，保证查询体验不受影响？









